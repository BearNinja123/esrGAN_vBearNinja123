{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esrGAN upscaling\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import joblib, os, cv2, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data\n",
    "os.chdir('train')\n",
    "rawX = joblib.load('lrImgs.sav') # rawX/X2 are just np arrays containing low-res/high-res images\n",
    "rawX2 = joblib.load('hrImgs.sav')\n",
    "os.chdir('..')\n",
    "\n",
    "# batch training data\n",
    "rawX = np.array(rawX)\n",
    "rawX2 = np.array(rawX2)\n",
    "\n",
    "m = rawX.shape[0]\n",
    "batchSize = 4\n",
    "X = tf.data.Dataset.from_tensor_slices(rawX).batch(batchSize)\n",
    "X2 = tf.data.Dataset.from_tensor_slices(rawX2).batch(batchSize)\n",
    "listX = list(X.as_numpy_iterator())\n",
    "listY = list(X2.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom blocks used in esrGAN\n",
    "\n",
    "class RB(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(RB, self).__init__()\n",
    "    self.cv1 = Conv2D(64, 3, padding='same')\n",
    "    self.cv2 = Conv2D(64, 3, padding='same')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    cv1 = self.cv1(inputs)\n",
    "    r1 = ReLU()(cv1ac)\n",
    "    cv2 = self.cv2(r1)\n",
    "    add1 = Add()([inputs, cv2])\n",
    "    return add1\n",
    "\n",
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(DenseBlock, self).__init__()\n",
    "    self.cv1 = Conv2D(64, 3, padding='same')\n",
    "    self.cv2 = Conv2D(64, 3, padding='same')\n",
    "    self.cv3 = Conv2D(64, 3, padding='same')\n",
    "    self.cv4 = Conv2D(64, 3, padding='same')\n",
    "    self.cv5 = Conv2D(64, 3, padding='same')\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    cv1 = self.cv1(inputs)\n",
    "    r1 = LeakyReLU()(cv1)\n",
    "    add1 = Concatenate()([inputs, r1])\n",
    "    \n",
    "    cv2 = self.cv2(add1)\n",
    "    r2 = LeakyReLU()(cv2)\n",
    "    add2 = Concatenate()([inputs, add1, r2])\n",
    "  \n",
    "    cv3 = self.cv3(add2)\n",
    "    r3 = LeakyReLU()(cv3)\n",
    "    add3 = Concatenate()([inputs, add1, add2, r3])\n",
    "  \n",
    "    cv4 = self.cv4(add3)\n",
    "    r4 = LeakyReLU()(cv4)\n",
    "    add4 = Concatenate()([inputs, add1, add2, add3, r4])\n",
    "  \n",
    "    cv5 = self.cv5(add4)\n",
    "    return cv5\n",
    "\n",
    "class RRDB(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(RRDB, self).__init__()\n",
    "    self.db1 = DenseBlock()\n",
    "    self.db2 = DenseBlock()\n",
    "    self.db3 = DenseBlock()\n",
    "    # i divided the variables by three so the activations wouldn't get too large\n",
    "    self.beta1 = tf.Variable(tf.random.normal(shape=()), trainable=True) / 3 \n",
    "    self.beta2 = tf.Variable(tf.random.normal(shape=()), trainable=True) / 3\n",
    "    self.beta3 = tf.Variable(tf.random.normal(shape=()), trainable=True) / 3\n",
    "    self.beta4 = tf.Variable(tf.random.normal(shape=()), trainable=True) / 3\n",
    "    self.beta5 = tf.Variable(tf.random.normal(shape=()), trainable=True) / 3\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    db1 = self.db1(inputs)\n",
    "    db1 = tf.scalar_mul(self.beta1, db1)\n",
    "    add1 = Add()([inputs, db1])\n",
    "  \n",
    "    db2 = self.db2(add1)\n",
    "    db2 = tf.scalar_mul(self.beta2, db2)\n",
    "    add2 = Add()([add1, db2])\n",
    "  \n",
    "    db3 = self.db3(add2)\n",
    "    db3 = tf.scalar_mul(self.beta3, db3)\n",
    "    add3 = Add()([add2, db3])\n",
    "\n",
    "    add3 = tf.scalar_mul(self.beta4, add3)\n",
    "    inps = tf.scalar_mul(self.beta5, inputs)\n",
    "    add4 = Add()([inps, add3])\n",
    "    return add4\n",
    "\n",
    "def buildDBlock(inp):\n",
    "  cv1 = Conv2D(32, 3, strides=2, padding='same')(inp)\n",
    "  bn1 = BatchNormalization()(cv1)\n",
    "  lr1 = LeakyReLU()(bn1)\n",
    "  d1 = Dropout(0.2)(lr1)\n",
    "  return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genGen():\n",
    "  inp = Input((32, 32, 3))\n",
    "  layer = Conv2D(64, 9, padding='same')(inp)\n",
    "\n",
    "  for i in range(3): # customize whichever block for your esrGAN\n",
    "    layer = RRDB()(layer)\n",
    "    #layer = DenseBlock()(layer)\n",
    "    #layer = RB()(layer)\n",
    "\n",
    "  layer = Conv2D(64, 3, padding='same')(layer)\n",
    "  layer = Concatenate()([cv1, layer])\n",
    "\n",
    "  layer = Conv2D(256, 3, padding='same')(layer)\n",
    "  layer = UpSampling2D()(layer)\n",
    "  layer = Conv2D(256, 3, padding='same')(layer)\n",
    "  layer = UpSampling2D()(layer)\n",
    "\n",
    "  layer = Conv2D(64, 3, padding='same')(layer)\n",
    "  output = Conv2D(3, 9, padding='same', activation='sigmoid')(layer) # did sigmoid since it seemed to work the best for me\n",
    "\n",
    "  generator = Model(inp, output, name='generator')\n",
    "  return generator\n",
    "\n",
    "def genDisc():\n",
    "  inp = Input((128, 128, 3))\n",
    "  layer = Conv2D(32, 3, padding='same')(inp)\n",
    "  layer = LeakyReLU()(layer)\n",
    "\n",
    "  for i in range(5):\n",
    "    layer = buildDBlock(layer)\n",
    "\n",
    "  flat = Flatten()(layer)\n",
    "  output = Dense(1, activation='linear')(flat)\n",
    "  discriminator = Model(inp, output, name='discriminator')\n",
    "\n",
    "  return discriminator\n",
    "\n",
    "def build_vgg():\n",
    "  vgg = VGG19(input_shape=(128, 128, 3), include_top=False, weights=\"imagenet\")\n",
    "  vgg.outputs = [vgg.layers[6].output] # vgg-54 is [15]\n",
    "  inputLayer = vgg.layers[0].output\n",
    "\n",
    "  return Model(inputLayer, vgg.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions - I hvan't found a way to get VGG activations before activation functions\n",
    "\n",
    "def dra(y1Pred, y2Pred):\n",
    "  return K.mean(tf.math.sigmoid(y1Pred - y2Pred))\n",
    "\n",
    "def discLoss(truePred, fakePred):\n",
    "  realLoss = -K.mean(tf.math.log(dra(truePred, fakePred)))\n",
    "  fakeLoss = -K.mean(tf.math.log(1 - dra(fakePred, truePred)))\n",
    "  return realLoss + fakeLoss\n",
    "\n",
    "def genLoss(truePred, fakePred, trueVGG, fakeVGG, y, fakeImgs, lb=5e-2, eta=1e-2):\n",
    "  mse = MeanSquaredError()\n",
    "  mae = MeanAbsoluteError()\n",
    "  percepLoss = mse(trueVGG, fakeVGG)\n",
    "\n",
    "  realLoss = -K.mean(tf.math.log(dra(truePred, fakePred)))\n",
    "  fakeLoss = -K.mean(tf.math.log(dra(fakePred, truePred)))\n",
    "  adLoss = lb * (realLoss + fakeLoss)\n",
    "\n",
    "  normLoss = eta * mae(y, fakeImgs)\n",
    "\n",
    "  return percepLoss + adLoss + normLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(batch, y, pastGenLoss):\n",
    "  with tf.GradientTape() as gtape, tf.GradientTape() as dtape:\n",
    "    fakes = genModel(batch, training=True)\n",
    "    truePreds = discModel(y, training=True)\n",
    "    fakePreds = discModel(fakes, training=True)\n",
    "    trueVGG = vgg(y, training=False)\n",
    "    fakeVGG = vgg(fakes, training=False)\n",
    "\n",
    "    dloss = discLoss(truePreds, fakePreds)\n",
    "    gloss = genLoss(truePreds, fakePreds, trueVGG, fakeVGG, y, fakes, lb=lb)\n",
    "\n",
    "    gradGen = gtape.gradient(gloss, genModel.trainable_variables)\n",
    "    genOpt.apply_gradients(zip(gradGen, genModel.trainable_variables))\n",
    "    if dloss > 15: # discriminator seems to train faster so i cripple it to not get mode collapse\n",
    "      gradDisc = dtape.gradient(dloss, discModel.trainable_variables)\n",
    "      discOpt.apply_gradients(zip(gradDisc, discModel.trainable_variables))\n",
    "    \n",
    "  return dloss, gloss\n",
    "\n",
    "def train(epochs, steps=1000):\n",
    "  global listX, listY, m, batchSize\n",
    "  for i in range(epochs):\n",
    "    dcost = 0\n",
    "    gcost = 0\n",
    "    gloss = 0\n",
    "    for batch in tqdm(range(steps)):\n",
    "      batchInd = np.random.randint(low=0, high=m//batchSize)\n",
    "      batchX = listX[batchInd]\n",
    "      batchY = listY[batchInd]\n",
    "      dloss, gloss = step(batchX, batchY, gloss)\n",
    "\n",
    "      dcost += dloss\n",
    "      gcost += gloss\n",
    "\n",
    "    print('\\n-----Epoch: {} | Discriminator Cost: {} | Generator Cost: {}-----\\n'.format(i, dcost, gcost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = build_vgg()\n",
    "vgg.trainable = False\n",
    "\n",
    "genModel = genGen()\n",
    "discModel = genDisc()\n",
    "\n",
    "# load trained models hereo\n",
    "#genModel = tf.keras.models.load_model('esrGAN_RRDB/gen')\n",
    "#discModel = tf.keras.models.load_model('esrGAN_RRDB/disc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genOpt = Adam(learning_rate=1e-4)\n",
    "discOpt = Adam(learning_rate=1e-4)\n",
    "\n",
    "# progressively increase lower lambda and lower learning rate to get sharper image quiality \n",
    "lb = 5e-3\n",
    "#lb = 5e-2\n",
    "#lb = 3e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training loop - show images and train\n",
    "\n",
    "while True:\n",
    "  rows, cols = 3, 5\n",
    "  fig = plt.figure(figsize=(30, 15))\n",
    "  axes = fig.subplots(rows, cols)\n",
    "  for i in range(cols):\n",
    "    if i % 2 == 0:\n",
    "      predInput = np.array([rawX[i]])\n",
    "      pred = genModel.predict(predInput)[0]\n",
    "    \n",
    "      axes[0][i].imshow(rawX[i])\n",
    "      axes[1][i].imshow(pred)\n",
    "      axes[2][i].imshow(rawX2[i])\n",
    "    else:\n",
    "      randI = np.random.randint(low=0, high=m)\n",
    "      predInput = np.array([rawX[randI]])\n",
    "      pred = genModel.predict(predInput)[0]\n",
    "\n",
    "      axes[0][i].imshow(rawX[randI])\n",
    "      axes[1][i].imshow(pred)\n",
    "      axes[2][i].imshow(rawX2[randI])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "genModel.save('esrGAN_RRDB_{}/gen'.format(now)\n",
    "discModel.save('esrGAN_RRDB_{}/disc'.format(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out memory\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
